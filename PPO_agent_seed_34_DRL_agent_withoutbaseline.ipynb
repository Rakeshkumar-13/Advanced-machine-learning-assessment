{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in c:\\programdata\\anaconda3\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.19.4)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (20.9)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.47.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.9.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.26.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.4.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (14.0.6)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.9.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (2.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2020.12.5)\n",
      "\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-gpu) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing the game**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym_super_mario_bros==7.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (7.3.0)\n",
      "Requirement already satisfied: nes_py in c:\\programdata\\anaconda3\\lib\\site-packages (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes_py) (0.25.2)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes_py) (4.59.0)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes_py) (1.5.21)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes_py) (1.20.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes_py) (1.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes_py) (4.12.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes_py) (0.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes_py) (3.4.1)\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system('pip install gym_super_mario_bros==7.3.0 nes_py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[3]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "# Importing the Joypad wrapper\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Importing the SIMPLIFIED controls\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[4]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "import gym_super_mario_bros\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym.wrappers import FrameStack\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[5]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grayscaling for observation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=self.observation_space.shape[:2], dtype=np.uint8)\n",
    "    def observation(self, observation):\n",
    "        transform = transforms.Grayscale()\n",
    "        return transform(torch.tensor(np.transpose(observation, (2, 0, 1)).copy(), dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resizing the observation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[7]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "    def observation(self, observation):\n",
    "        transformations = transforms.Compose([transforms.Resize(self.shape), transforms.Normalize(0, 255)])\n",
    "        return transformations(observation).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros2-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros2-v0')\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "env.seed(34)\n",
    "env.action_space.seed(34)\n",
    "torch.manual_seed(34)\n",
    "torch.random.manual_seed(34)\n",
    "np.random.seed(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[8]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.action_space.n)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    def forward(self, obs):\n",
    "        return Categorical(logits=self.actor(obs)), self.critic(obs).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[9]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOSolver:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.gamma = 0.95\n",
    "        self.lamda = 0.95\n",
    "        self.worker_steps = 4096\n",
    "        self.n_mini_batch = 4\n",
    "        self.epochs = 30\n",
    "        self.save_directory = r\"C:\\Users\\Computing\\Desktop\\PPO seed =34\"\n",
    "        self.batch_size = self.worker_steps\n",
    "        self.mini_batch_size = self.batch_size // self.n_mini_batch\n",
    "        self.obs = env.reset().__array__()\n",
    "        self.policy = Model().to(device)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.actor.parameters(), 'lr': 0.00025},\n",
    "            {'params': self.policy.critic.parameters(), 'lr': 0.001}\n",
    "        ], eps=1e-4)\n",
    "        self.policy_old = Model().to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.all_episode_rewards = []\n",
    "        self.all_mean_rewards = []\n",
    "        self.episode = 0\n",
    "    def save_checkpoint(self):\n",
    "        filename = os.path.join(self.save_directory, 'checkpoint_{}.pth'.format(self.episode))\n",
    "        torch.save(self.policy_old.state_dict(), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))\n",
    "    def load_checkpoint(self, filename):\n",
    "        self.policy.load_state_dict(torch.load(os.path.join(self.save_directory, filename)))\n",
    "        self.policy_old.load_state_dict(torch.load(os.path.join(self.save_directory, filename)))\n",
    "        print('Resuming training from checkpoint \\'{}\\'.'.format(filename))\n",
    "    def sample(self):\n",
    "        rewards = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        actions = np.zeros(self.worker_steps, dtype=np.int32)\n",
    "        done = np.zeros(self.worker_steps, dtype=bool)\n",
    "        obs = np.zeros((self.worker_steps, 4, 84, 84), dtype=np.float32)\n",
    "        log_pis = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        values = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        for t in range(self.worker_steps):\n",
    "            with torch.no_grad():\n",
    "                obs[t] = self.obs\n",
    "                pi, v = self.policy_old(torch.tensor(self.obs, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "                values[t] = v.cpu().numpy()\n",
    "                a = pi.sample()\n",
    "                actions[t] = a.cpu().numpy()\n",
    "                log_pis[t] = pi.log_prob(a).cpu().numpy()\n",
    "            self.obs, rewards[t], done[t], _ = env.step(actions[t])\n",
    "            self.obs = self.obs.__array__()\n",
    "            env.render()\n",
    "            self.rewards.append(rewards[t])\n",
    "            if done[t]:\n",
    "                self.episode += 1\n",
    "                self.all_episode_rewards.append(np.sum(self.rewards))\n",
    "                self.rewards = []\n",
    "                env.reset()\n",
    "                if self.episode % 10 == 0:\n",
    "                    print('Episode: {}, average reward: {}'.format(self.episode, np.mean(self.all_episode_rewards[-10:])))\n",
    "                    self.all_mean_rewards.append(np.mean(self.all_episode_rewards[-10:]))\n",
    "                    plt.plot(self.all_mean_rewards)\n",
    "                    plt.savefig(\"{}/mean_reward_{}.png\".format(self.save_directory, self.episode))\n",
    "                    plt.clf()\n",
    "                    self.save_checkpoint()\n",
    "        returns, advantages = self.calculate_advantages(done, rewards, values)\n",
    "        return {\n",
    "            'obs': torch.tensor(obs.reshape(obs.shape[0], *obs.shape[1:]), dtype=torch.float32, device=device),\n",
    "            'actions': torch.tensor(actions, device=device),\n",
    "            'values': torch.tensor(values, device=device),\n",
    "            'log_pis': torch.tensor(log_pis, device=device),\n",
    "            'advantages': torch.tensor(advantages, device=device, dtype=torch.float32),\n",
    "            'returns': torch.tensor(returns, device=device, dtype=torch.float32)\n",
    "        }\n",
    "    def calculate_advantages(self, done, rewards, values):\n",
    "        _, last_value = self.policy_old(torch.tensor(self.obs, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "        last_value = last_value.cpu().data.numpy()\n",
    "        values = np.append(values, last_value)\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            mask = 1.0 - done[i]\n",
    "            delta = rewards[i] + self.gamma * values[i + 1] * mask - values[i]\n",
    "            gae = delta + self.gamma * self.lamda * mask * gae\n",
    "            returns.insert(0, gae + values[i])\n",
    "        adv = np.array(returns) - values[:-1]\n",
    "        return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
    "    def train(self, samples, clip_range):\n",
    "        indexes = torch.randperm(self.batch_size)\n",
    "        for start in range(0, self.batch_size, self.mini_batch_size):\n",
    "            end = start + self.mini_batch_size\n",
    "            mini_batch_indexes = indexes[start: end]\n",
    "            mini_batch = {}\n",
    "            for k, v in samples.items():\n",
    "                mini_batch[k] = v[mini_batch_indexes]\n",
    "            for _ in range(self.epochs):\n",
    "                loss = self.calculate_loss(clip_range=clip_range, samples=mini_batch)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "    def calculate_loss(self, samples, clip_range):\n",
    "        sampled_returns = samples['returns']\n",
    "        sampled_advantages = samples['advantages']\n",
    "        pi, value = self.policy(samples['obs'])\n",
    "        ratio = torch.exp(pi.log_prob(samples['actions']) - samples['log_pis'])\n",
    "        clipped_ratio = ratio.clamp(min=1.0 - clip_range, max=1.0 + clip_range)\n",
    "        policy_reward = torch.min(ratio * sampled_advantages, clipped_ratio * sampled_advantages)\n",
    "        entropy_bonus = pi.entropy()\n",
    "        vf_loss = self.mse_loss(value, sampled_returns)\n",
    "        loss = -policy_reward + 0.5 * vf_loss - 0.01 * entropy_bonus\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using PPO agent as solver**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[10]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, average reward: 1632.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_10.pth'\n",
      "Episode: 20, average reward: 1765.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_20.pth'\n",
      "Episode: 30, average reward: 1638.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_30.pth'\n",
      "Episode: 40, average reward: 1846.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_40.pth'\n",
      "Episode: 50, average reward: 1829.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_50.pth'\n",
      "Episode: 60, average reward: 1749.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_60.pth'\n",
      "Episode: 70, average reward: 1986.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_70.pth'\n",
      "Episode: 80, average reward: 2152.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_80.pth'\n",
      "Episode: 90, average reward: 2231.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_90.pth'\n",
      "Episode: 100, average reward: 2420.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_100.pth'\n",
      "Episode: 110, average reward: 2412.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_110.pth'\n",
      "Episode: 120, average reward: 2140.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_120.pth'\n",
      "Episode: 130, average reward: 2259.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_130.pth'\n",
      "Episode: 140, average reward: 2342.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_140.pth'\n",
      "Episode: 150, average reward: 2393.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_150.pth'\n",
      "Episode: 160, average reward: 2506.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_160.pth'\n",
      "Episode: 170, average reward: 2357.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_170.pth'\n",
      "Episode: 180, average reward: 2512.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_180.pth'\n",
      "Episode: 190, average reward: 2290.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_190.pth'\n",
      "Episode: 200, average reward: 2391.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_200.pth'\n",
      "Episode: 210, average reward: 2488.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_210.pth'\n",
      "Episode: 220, average reward: 2453.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_220.pth'\n",
      "Episode: 230, average reward: 2282.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_230.pth'\n",
      "Episode: 240, average reward: 2509.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_240.pth'\n",
      "Episode: 250, average reward: 2344.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_250.pth'\n",
      "Episode: 260, average reward: 2196.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_260.pth'\n",
      "Episode: 270, average reward: 2428.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_270.pth'\n",
      "Episode: 280, average reward: 2347.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_280.pth'\n",
      "Episode: 290, average reward: 2481.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_290.pth'\n",
      "Episode: 300, average reward: 2264.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_300.pth'\n",
      "Episode: 310, average reward: 2466.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_310.pth'\n",
      "Episode: 320, average reward: 2356.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_320.pth'\n",
      "Episode: 330, average reward: 2691.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_330.pth'\n",
      "Episode: 340, average reward: 2400.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_340.pth'\n",
      "Episode: 350, average reward: 2492.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_350.pth'\n",
      "Episode: 360, average reward: 2468.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_360.pth'\n",
      "Episode: 370, average reward: 2090.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_370.pth'\n",
      "Episode: 380, average reward: 2451.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_380.pth'\n",
      "Episode: 390, average reward: 2292.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_390.pth'\n",
      "Episode: 400, average reward: 2465.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_400.pth'\n",
      "Episode: 410, average reward: 2438.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_410.pth'\n",
      "Episode: 420, average reward: 2590.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_420.pth'\n",
      "Episode: 430, average reward: 2666.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_430.pth'\n",
      "Episode: 440, average reward: 2740.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_440.pth'\n",
      "Episode: 450, average reward: 2662.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_450.pth'\n",
      "Episode: 460, average reward: 2534.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_460.pth'\n",
      "Episode: 470, average reward: 2663.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_470.pth'\n",
      "Episode: 480, average reward: 2782.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_480.pth'\n",
      "Episode: 490, average reward: 2472.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_490.pth'\n",
      "Episode: 500, average reward: 2783.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_500.pth'\n",
      "Episode: 510, average reward: 2499.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_510.pth'\n",
      "Episode: 520, average reward: 2813.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_520.pth'\n",
      "Episode: 530, average reward: 2440.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_530.pth'\n",
      "Episode: 540, average reward: 2637.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_540.pth'\n",
      "Episode: 550, average reward: 2673.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_550.pth'\n",
      "Episode: 560, average reward: 2576.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_560.pth'\n",
      "Episode: 570, average reward: 2695.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_570.pth'\n",
      "Episode: 580, average reward: 2782.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_580.pth'\n",
      "Episode: 590, average reward: 2258.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_590.pth'\n",
      "Episode: 600, average reward: 2590.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_600.pth'\n",
      "Episode: 610, average reward: 2719.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_610.pth'\n",
      "Episode: 620, average reward: 2722.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO seed =34\\checkpoint_620.pth'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0093ac6b592a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPOSolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-a9fafecface7>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mlog_pis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \"\"\"\n\u001b[0;32m    176\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         )\n\u001b[0;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[0mstep_returns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[0mstep_returns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-e080007c7d8d>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nes_py\\wrappers\\joypad_space.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# take the step and record the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_action_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \"\"\"\n\u001b[0;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \"\"\"\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mstep_returns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nes_py\\nes_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrollers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;31m# pass the action to the emulator as an unsigned byte\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m         \u001b[1;31m# get the reward for this step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver = PPOSolver()\n",
    "while True:\n",
    "    solver.train(solver.sample(), 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
